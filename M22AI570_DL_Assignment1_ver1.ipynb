{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1P74B38VyRd_Iz3oqDvBMvzgLZ0mq874S",
      "authorship_tag": "ABX9TyPzZmeRonP21kITcuDSZinR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "''' Reference : https://colab.research.google.com/drive/16-Al3cM1PnKjYK9fY7vV9H2QdM_dMPAG#scrollTo=0ad8hHvAlGdA '''"
      ],
      "metadata": {
        "id": "4pdI4AI3nnTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries"
      ],
      "metadata": {
        "id": "rQ7AmrsZg5AU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Nlw0duAjTmul"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "from sklearn import svm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a pre-trained CNN model"
      ],
      "metadata": {
        "id": "buVMa0S-_X1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download a pre-trained CNN model\n",
        "model = models.resnet50(pretrained=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "JqygUN-4BUke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define data transformation\n",
        "'''The specific values used in the transforms.Normalize transformation are derived from the ImageNet dataset statistics. These values are commonly used for pre-trained models trained on the ImageNet dataset.'''Extract features from the last fully-connected layer"
      ],
      "metadata": {
        "id": "ApZgNDjn_jxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "vOsn-w9H_h_z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the last fully-connected layer\n",
        "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])"
      ],
      "metadata": {
        "id": "xcu3OhnviFJW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the VOC train dataset"
      ],
      "metadata": {
        "id": "nZl7f7IpAbda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pascal_voc2007_data(image_root, split='train'):\n",
        "  from torchvision import datasets\n",
        "  image_root='/content/drive/MyDrive/DL_Assignment/dataset/VOCtrainval_06-Nov-2007/'\n",
        "  train_dataset = datasets.VOCDetection(image_root, year='2007', image_set=split, download=False)  \n",
        "  return train_dataset"
      ],
      "metadata": {
        "id": "9Wy0JATaM_wr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_pascal_voc2007_data('/content', 'trainval')\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')\n",
        "\n",
        "# an example on the raw annotation\n",
        "import json\n",
        "print(json.dumps(train_dataset[1][1]['annotation'], indent=2))\n",
        "print(json.dumps(val_dataset[1][1]['annotation'], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E55pdfF6HE9a",
        "outputId": "3ba61c77-8c2c-4197-9147-eda43143e214"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"folder\": \"VOC2007\",\n",
            "  \"filename\": \"000007.jpg\",\n",
            "  \"source\": {\n",
            "    \"database\": \"The VOC2007 Database\",\n",
            "    \"annotation\": \"PASCAL VOC2007\",\n",
            "    \"image\": \"flickr\",\n",
            "    \"flickrid\": \"194179466\"\n",
            "  },\n",
            "  \"owner\": {\n",
            "    \"flickrid\": \"monsieurrompu\",\n",
            "    \"name\": \"Thom Zemanek\"\n",
            "  },\n",
            "  \"size\": {\n",
            "    \"width\": \"500\",\n",
            "    \"height\": \"333\",\n",
            "    \"depth\": \"3\"\n",
            "  },\n",
            "  \"segmented\": \"0\",\n",
            "  \"object\": [\n",
            "    {\n",
            "      \"name\": \"car\",\n",
            "      \"pose\": \"Unspecified\",\n",
            "      \"truncated\": \"1\",\n",
            "      \"difficult\": \"0\",\n",
            "      \"bndbox\": {\n",
            "        \"xmin\": \"141\",\n",
            "        \"ymin\": \"50\",\n",
            "        \"xmax\": \"500\",\n",
            "        \"ymax\": \"330\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "{\n",
            "  \"folder\": \"VOC2007\",\n",
            "  \"filename\": \"000007.jpg\",\n",
            "  \"source\": {\n",
            "    \"database\": \"The VOC2007 Database\",\n",
            "    \"annotation\": \"PASCAL VOC2007\",\n",
            "    \"image\": \"flickr\",\n",
            "    \"flickrid\": \"194179466\"\n",
            "  },\n",
            "  \"owner\": {\n",
            "    \"flickrid\": \"monsieurrompu\",\n",
            "    \"name\": \"Thom Zemanek\"\n",
            "  },\n",
            "  \"size\": {\n",
            "    \"width\": \"500\",\n",
            "    \"height\": \"333\",\n",
            "    \"depth\": \"3\"\n",
            "  },\n",
            "  \"segmented\": \"0\",\n",
            "  \"object\": [\n",
            "    {\n",
            "      \"name\": \"car\",\n",
            "      \"pose\": \"Unspecified\",\n",
            "      \"truncated\": \"1\",\n",
            "      \"difficult\": \"0\",\n",
            "      \"bndbox\": {\n",
            "        \"xmin\": \"141\",\n",
            "        \"ymin\": \"50\",\n",
            "        \"xmax\": \"500\",\n",
            "        \"ymax\": \"330\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5MjBX9bkBtA"
      },
      "source": [
        "''' In order to use these annotations to train our model, we need to convert this nested dictionary data structure into a set of PyTorch tensors.\n",
        "\n",
        "We also need to preprocess the image, converting it to a PyTorch tensor and resizing it to 224x224. Real object detection systems typically work with much higher-resolution images, but we will use a low resolution for computational efficiency in this assignment.\n",
        "\n",
        "We also want to train our models using minibatches of data, so we need to group the annotations from several images into minibatches.\n",
        "\n",
        "I perform both of these functions by using a customized PyTorch DataLoader'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pascal_voc2007_loader(dataset, batch_size, num_workers=0):\n",
        "  \"\"\"\n",
        "  Data loader for Pascal VOC 2007.\n",
        "  https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  from torch.utils.data import DataLoader\n",
        "  # turn off shuffle so we can index the original image\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False, pin_memory=True,\n",
        "                            num_workers=num_workers,\n",
        "                            collate_fn=voc_collate_fn)\n",
        "  return train_loader\n",
        "\n",
        "\n",
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "def voc_collate_fn(batch_lst, reshape_size=224):\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((reshape_size, reshape_size)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "    \n",
        "    batch_size = len(batch_lst)\n",
        "    \n",
        "    img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n",
        "    \n",
        "    max_num_box = max(len(batch_lst[i][1]['annotation']['object']) \\\n",
        "                      for i in range(batch_size))\n",
        "\n",
        "    box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)\n",
        "    w_list = []\n",
        "    h_list = []\n",
        "    img_id_list = []\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      img, ann = batch_lst[i]\n",
        "      w_list.append(img.size[0]) # image width\n",
        "      h_list.append(img.size[1]) # image height\n",
        "      img_id_list.append(ann['annotation']['filename'])\n",
        "      img_batch[i] = preprocess(img)\n",
        "      all_bbox = ann['annotation']['object']\n",
        "      if type(all_bbox) == dict: # inconsistency in the annotation file\n",
        "        all_bbox = [all_bbox]\n",
        "      for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "        bbox = one_bbox['bndbox']\n",
        "        obj_cls = one_bbox['name']\n",
        "        box_batch[i][bbox_idx] = torch.Tensor([float(bbox['xmin']), float(bbox['ymin']),\n",
        "          float(bbox['xmax']), float(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "    \n",
        "    h_batch = torch.tensor(h_list)\n",
        "    w_batch = torch.tensor(w_list)\n",
        "\n",
        "    return img_batch, box_batch, w_batch, h_batch, img_id_list"
      ],
      "metadata": {
        "id": "gd8qOURROzoH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''Training with the entire PASCAL VOC will be too computationally expensive for this homework assignment, so I have taken subsample the dataset by wrapping each Dataset object in a Subset object'''"
      ],
      "metadata": {
        "id": "y7oFK4SRO_79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 2500)) # use 2500 samples for training\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 50)\n",
        "val_loader = pascal_voc2007_loader(val_dataset, 50)"
      ],
      "metadata": {
        "id": "deJNmNjiO07N"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the features for the training data"
      ],
      "metadata": {
        "id": "FhO0uffEQACt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = []\n",
        "train_labels = []\n",
        "\n",
        "#model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, target in train_dataset:\n",
        "        try:\n",
        "            image = transform(image)  # Apply transform to convert Image to Tensor\n",
        "            feature = model(torch.unsqueeze(image, 0))\n",
        "            train_features.append(feature.squeeze().numpy())\n",
        "            train_labels.append(target[\"annotation\"][\"object\"][0][\"name\"])\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error: {e}. Skipping image.\")\n",
        "\n",
        "train_features = np.array(train_features)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vGdWabtEP6Mo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a pre-trained CNN model"
      ],
      "metadata": {
        "id": "VePT2TJyfxLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' model = models.resnet50(pretrained=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval() '''"
      ],
      "metadata": {
        "id": "QggYjT1DBITu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the feature from last fully connected layer"
      ],
      "metadata": {
        "id": "6RhgSrUlBwS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the last fully-connected layer\n",
        "'''feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])'''\n"
      ],
      "metadata": {
        "id": "G6bQV6BfZkrZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten the features if they have more than 2 dimensions"
      ],
      "metadata": {
        "id": "liOQmRVqCD1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the features if they have more than 2 dimensions\n",
        "if train_features.ndim > 2:\n",
        "    train_features = train_features.reshape(train_features.shape[0], -1)"
      ],
      "metadata": {
        "id": "NDIuSlbXCCMH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train binary one-vs.-rest SVM classifiers"
      ],
      "metadata": {
        "id": "BgGsbj2BCS_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Train binary one-vs.-rest SVM classifiers\n",
        "svm_classifiers = []\n",
        "classes = np.unique(train_labels)\n",
        "print(classes)\n",
        "\n",
        "for class_idx in classes:\n",
        "    # Create binary labels for one class vs. rest\n",
        "    binary_labels = np.where(train_labels == class_idx, 1, 0)\n",
        "    \n",
        "    # Train SVM classifier for the current class\n",
        "    svm_classifier = svm.SVC(kernel='linear')\n",
        "    svm_classifier.fit(train_features, binary_labels)\n",
        "    svm_classifiers.append(svm_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIdTluMrCP4r",
        "outputId": "ad1662b6-5ed2-4ac6-eaa7-d7eed3202133"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aeroplane' 'bicycle' 'bird' 'boat' 'bottle' 'bus' 'car' 'cat' 'chair'\n",
            " 'cow' 'diningtable' 'dog' 'horse' 'motorbike' 'person' 'pottedplant'\n",
            " 'sheep' 'sofa' 'train' 'tvmonitor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate classification accuracy on the validation set"
      ],
      "metadata": {
        "id": "P2UsUfTpWrrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Evaluate classification accuracy on the validation set\n",
        "##val_dataset = VOCDetection(data_dir, year=\"2007\", image_set=\"test\", download=False, transform=transform)\n",
        "subset_size = 500  # Number of images in the subset. Evaluate classification accuracy on a subset of the test dataset\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')\n",
        "\n",
        "val_features = []\n",
        "val_labels = []\n",
        "subset_counter = 0  # Counter to keep track of the number of images added to the subset\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image, target in val_dataset:\n",
        "        try:\n",
        "            image = transform(image)  # Apply transform to convert Image to Tensor\n",
        "            feature = model(torch.unsqueeze(image, 0))\n",
        "            val_features.append(feature.squeeze().numpy())\n",
        "            val_labels.append(target[\"annotation\"][\"object\"][0][\"name\"])\n",
        "            subset_counter += 1\n",
        "\n",
        "            if subset_counter >= subset_size:\n",
        "              break\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error: {e}. Skipping image.\")\n",
        "\n",
        "val_features = np.array(val_features)\n",
        "val_labels = np.array(val_labels)\n",
        "print(\"Validation Features:\")\n",
        "print(val_features)\n",
        "print(\"Validation Labels:\")\n",
        "print(val_labels)\n",
        "\n",
        "\n",
        "if val_features.ndim > 2:\n",
        "    val_features = val_features.reshape(val_features.shape[0], -1)\n",
        "\n",
        "val_predictions = []\n",
        "\n",
        "for feature in val_features:\n",
        "    predictions = []\n",
        "    for classifier in svm_classifiers:\n",
        "        prediction = classifier.predict(feature.reshape(1, -1))\n",
        "        predictions.append(prediction)\n",
        "    val_predictions.append(predictions)\n",
        "\n",
        "val_predictions = np.array(val_predictions)\n",
        "val_labels = np.array(val_labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFvaEPUIWm4j",
        "outputId": "340ca508-b55c-424d-90f8-cafaaea74009"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Features:\n",
            "[[-2.457909   -0.7341722  -1.4729944  ... -0.47497892  0.49226993\n",
            "  -1.7070079 ]\n",
            " [-0.20414515 -1.2274386  -2.2608266  ... -1.1279682   0.53636587\n",
            "  -0.0572966 ]\n",
            " [-0.23880373 -2.3812976  -0.5844568  ... -2.397542    2.1173797\n",
            "  -0.1096619 ]\n",
            " ...\n",
            " [ 0.5107952   1.5010961  -2.0242476  ...  0.6346948   1.0828748\n",
            "   0.8873813 ]\n",
            " [-1.214551    0.5656842   2.0793295  ...  0.33333498  4.501563\n",
            "  -1.1175675 ]\n",
            " [-1.5068824  -1.7804626  -2.9174366  ... -1.0629003   2.1781566\n",
            "   2.6822975 ]]\n",
            "Validation Labels:\n",
            "['chair' 'car' 'horse' 'bicycle' 'cat' 'car' 'dog' 'train' 'bicycle'\n",
            " 'tvmonitor' 'tvmonitor' 'bird' 'chair' 'motorbike' 'pottedplant' 'car'\n",
            " 'dog' 'dog' 'motorbike' 'sofa' 'dog' 'train' 'cat' 'chair' 'sofa' 'sheep'\n",
            " 'bird' 'person' 'person' 'aeroplane' 'cat' 'dog' 'tvmonitor' 'train'\n",
            " 'bicycle' 'boat' 'car' 'bus' 'car' 'cat' 'person' 'horse' 'sofa' 'bird'\n",
            " 'person' 'car' 'bottle' 'diningtable' 'car' 'boat' 'bus' 'chair'\n",
            " 'diningtable' 'car' 'cow' 'horse' 'cat' 'dog' 'car' 'tvmonitor'\n",
            " 'diningtable' 'cow' 'sheep' 'bird' 'boat' 'sheep' 'car' 'dog' 'chair'\n",
            " 'car' 'tvmonitor' 'dog' 'aeroplane' 'bottle' 'train' 'person' 'person'\n",
            " 'car' 'person' 'car' 'car' 'tvmonitor' 'bicycle' 'person' 'car' 'person'\n",
            " 'person' 'person' 'horse' 'car' 'horse' 'dog' 'person' 'bird' 'car'\n",
            " 'person' 'cat' 'bird' 'car' 'motorbike' 'bicycle' 'diningtable' 'car'\n",
            " 'cat' 'cow' 'cow' 'motorbike' 'car' 'bird' 'motorbike' 'person' 'chair'\n",
            " 'horse' 'train' 'person' 'diningtable' 'chair' 'chair' 'car' 'cow' 'cow'\n",
            " 'person' 'train' 'horse' 'cat' 'cow' 'chair' 'car' 'bottle' 'person'\n",
            " 'bicycle' 'dog' 'car' 'boat' 'aeroplane' 'bicycle' 'person' 'person'\n",
            " 'person' 'person' 'train' 'car' 'person' 'boat' 'diningtable' 'car'\n",
            " 'sheep' 'dog' 'sheep' 'person' 'tvmonitor' 'chair' 'pottedplant' 'boat'\n",
            " 'aeroplane' 'diningtable' 'bird' 'person' 'car' 'bicycle' 'bird'\n",
            " 'aeroplane' 'car' 'dog' 'sofa' 'sofa' 'bus' 'horse' 'train' 'sheep'\n",
            " 'person' 'bird' 'chair' 'motorbike' 'bicycle' 'person' 'horse' 'cat'\n",
            " 'tvmonitor' 'train' 'boat' 'bird' 'chair' 'dog' 'cow' 'aeroplane'\n",
            " 'aeroplane' 'boat' 'dog' 'bus' 'person' 'sofa' 'bicycle' 'dog' 'horse'\n",
            " 'aeroplane' 'chair' 'dog' 'car' 'dog' 'chair' 'bus' 'cow' 'car' 'train'\n",
            " 'cat' 'dog' 'horse' 'car' 'person' 'person' 'cow' 'person' 'aeroplane'\n",
            " 'bicycle' 'boat' 'train' 'bird' 'cow' 'dog' 'tvmonitor' 'diningtable'\n",
            " 'person' 'sofa' 'bus' 'cat' 'person' 'train' 'sofa' 'aeroplane' 'person'\n",
            " 'cat' 'bicycle' 'horse' 'bird' 'dog' 'bicycle' 'person' 'aeroplane'\n",
            " 'person' 'car' 'pottedplant' 'person' 'train' 'cat' 'tvmonitor' 'person'\n",
            " 'boat' 'aeroplane' 'car' 'car' 'train' 'dog' 'tvmonitor' 'bicycle' 'bus'\n",
            " 'car' 'bicycle' 'aeroplane' 'bird' 'cat' 'aeroplane' 'motorbike' 'sofa'\n",
            " 'chair' 'chair' 'bird' 'dog' 'bottle' 'car' 'dog' 'dog' 'bird'\n",
            " 'tvmonitor' 'person' 'car' 'sofa' 'bird' 'train' 'horse' 'person' 'dog'\n",
            " 'car' 'person' 'aeroplane' 'dog' 'aeroplane' 'person' 'cat' 'boat' 'bus'\n",
            " 'car' 'boat' 'person' 'pottedplant' 'cow' 'aeroplane' 'dog' 'car' 'sofa'\n",
            " 'sofa' 'aeroplane' 'train' 'diningtable' 'tvmonitor' 'person' 'person'\n",
            " 'boat' 'bicycle' 'cat' 'bicycle' 'chair' 'dog' 'motorbike' 'train'\n",
            " 'motorbike' 'dog' 'person' 'cat' 'bicycle' 'train' 'bicycle' 'dog'\n",
            " 'chair' 'cat' 'horse' 'horse' 'boat' 'bird' 'person' 'person' 'bird'\n",
            " 'person' 'motorbike' 'bottle' 'cat' 'aeroplane' 'car' 'person'\n",
            " 'motorbike' 'aeroplane' 'bird' 'car' 'bottle' 'dog' 'person' 'sheep'\n",
            " 'bicycle' 'dog' 'dog' 'car' 'horse' 'bird' 'tvmonitor' 'chair' 'person'\n",
            " 'car' 'bird' 'person' 'car' 'bird' 'train' 'motorbike' 'cat' 'dog'\n",
            " 'aeroplane' 'bird' 'diningtable' 'car' 'diningtable' 'chair' 'cat' 'sofa'\n",
            " 'bus' 'person' 'boat' 'pottedplant' 'chair' 'car' 'dog' 'car' 'motorbike'\n",
            " 'boat' 'boat' 'dog' 'person' 'boat' 'person' 'person' 'person' 'bottle'\n",
            " 'horse' 'chair' 'tvmonitor' 'person' 'cat' 'tvmonitor' 'person'\n",
            " 'aeroplane' 'bicycle' 'dog' 'person' 'train' 'person' 'chair' 'cat'\n",
            " 'tvmonitor' 'bus' 'sheep' 'person' 'person' 'cow' 'motorbike' 'car'\n",
            " 'horse' 'horse' 'aeroplane' 'car' 'dog' 'bird' 'train' 'dog'\n",
            " 'diningtable' 'tvmonitor' 'person' 'bird' 'person' 'person' 'bus' 'car'\n",
            " 'dog' 'horse' 'tvmonitor' 'bicycle' 'horse' 'horse' 'car' 'cat' 'person'\n",
            " 'cat' 'diningtable' 'chair' 'person' 'train' 'dog' 'bird' 'person'\n",
            " 'person' 'person' 'person' 'sofa' 'cow' 'motorbike' 'horse' 'cat' 'bird'\n",
            " 'tvmonitor' 'person' 'bicycle' 'person' 'aeroplane' 'boat' 'person'\n",
            " 'bird' 'chair' 'car' 'sheep' 'tvmonitor' 'car' 'sofa' 'sofa' 'chair'\n",
            " 'cat' 'car' 'chair' 'train' 'aeroplane' 'person' 'sofa' 'horse' 'car'\n",
            " 'person' 'bird' 'cat' 'aeroplane' 'cat' 'car' 'motorbike' 'person' 'bus'\n",
            " 'bird' 'bottle' 'dog' 'boat' 'cat' 'boat' 'dog' 'car' 'cow' 'motorbike']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate accuracy and print confusion matrix"
      ],
      "metadata": {
        "id": "txcb7XSIelqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "total_predictions = len(val_predictions)\n",
        "\n",
        "for predictions, label in zip(val_predictions, val_labels):\n",
        "    if label in predictions:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyKmZa0vdHM5",
        "outputId": "247d8c78-6546-4b28-bfeb-0f9e82c408a3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-89d4bee8d5d0>:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if label in predictions:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Convert labels to a consistent data type\n",
        "val_labels_flat = np.array(val_labels_flat)\n",
        "\n",
        "# Flatten the predictions and convert them to the same data type as labels\n",
        "val_predictions_flat = np.concatenate(val_predictions).astype(val_labels_flat.dtype)[:subset_size]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(val_labels_flat, val_predictions_flat)\n",
        "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(val_labels_flat, val_predictions_flat)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJsPKB4QzwnD",
        "outputId": "8ee8403b-3bcb-4ec6-8a59-ee843a2e169e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.00%\n",
            "Confusion Matrix:\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [12  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ]
        }
      ]
    }
  ]
}